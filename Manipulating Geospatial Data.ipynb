{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afd040-3901-481e-b594-1808c4e7f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium import Marker\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0f35d-f779-4107-9a6e-bd3c3ee04980",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In the code cell above, Nominatim refers to the geocoding software that will be used to generate locations.\n",
    "\n",
    "We begin by instantiating the geocoder. Then, we need only apply the name or address as a Python string. (In this case, we supply \"Pyramid of Khufu\", also known as the Great Pyramid of Giza.)\n",
    "\n",
    "If the geocoding is successful, it returns a geopy.location.Location object with two important attributes:\n",
    "\n",
    "the \"point\" attribute contains the (latitude, longitude) location, and\n",
    "the \"address\" attribute contains the full address.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc6863-cd0e-4471-bae6-ef4580461789",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"kaggle_learn\")\n",
    "location = geolocator.geocode(\"Pyramid of Khufu\")\n",
    "\n",
    "print(location.point)\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a9a00-7a00-4991-a8cc-94818764ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value for the \"point\" attribute is a geopy.point.Point object, and we can get the latitude and longitude from the latitude and longitude attributes, respectively.\n",
    "\n",
    "point = location.point\n",
    "print(\"Latitude:\", point.latitude)\n",
    "print(\"Longitude:\", point.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1331c6c-1c36-45be-9ab3-959bb413f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's often the case that we'll need to geocode many different addresses. For instance, say we want to obtain the locations of 100 top universities in Europe.\n",
    "\n",
    "universities = pd.read_csv(\"../input/geospatial-learn-course-data/top_universities.csv\")\n",
    "universities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5331799-d656-4108-8709-84fc3f769d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can use a lambda function to apply the geocoder to every row in the DataFrame.\n",
    "\n",
    "def my_geocoder(row):\n",
    "    try:\n",
    "        point = geolocator.geocode(row).point\n",
    "        return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "universities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis=1)\n",
    "\n",
    "print(\"{}% of addresses were geocoded!\".format(\n",
    "    (1 - sum(np.isnan(universities[\"Latitude\"])) / len(universities)) * 100))\n",
    "\n",
    "# Drop universities that were not successfully geocoded\n",
    "universities = universities.loc[~np.isnan(universities[\"Latitude\"])]\n",
    "universities = gpd.GeoDataFrame(\n",
    "    universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude))\n",
    "universities.crs = {'init': 'epsg:4326'}\n",
    "universities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700573c-5450-4c7b-8a40-21f01a8c87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all of the locations that were returned by the geocoder. \n",
    "\n",
    "# Create a map\n",
    "m = folium.Map(location=[54, 15], tiles='openstreetmap', zoom_start=2)\n",
    "\n",
    "# Add points to the map\n",
    "for idx, row in universities.iterrows():\n",
    "    Marker([row['Latitude'], row['Longitude']], popup=row['Name']).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d3bb1-737c-4f31-be86-24d94f54dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table joins\n",
    "# Combine data from different sources.\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "europe = world.loc[world.continent == 'Europe'].reset_index(drop=True)\n",
    "\n",
    "europe_stats = europe[[\"name\", \"pop_est\", \"gdp_md_est\"]]\n",
    "europe_boundaries = europe[[\"name\", \"geometry\"]]\n",
    "europe_boundaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398be2e-6e2f-4e7f-8f36-486248055325",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e1e52-6f4e-48f8-9b87-25d918fa3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the attribute join in the code cell below. The on argument is set to the column name that is used to match rows in europe_boundaries to rows in europe_stats.\n",
    "\n",
    "# Use an attribute join to merge data about countries in Europe\n",
    "europe = europe_boundaries.merge(europe_stats, on=\"name\")\n",
    "europe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468dd35-3705-4428-9799-0c15197a47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can use a spatial join to match each university to its corresponding country. We do this with gpd.sjoin().\n",
    "\n",
    "# Use spatial join to match universities to countries in Europe\n",
    "european_universities = gpd.sjoin(universities, europe)\n",
    "\n",
    "# Investigate the result\n",
    "print(\"We located {} universities.\".format(len(universities)))\n",
    "print(\"Only {} of the universities were located in Europe (in {} different countries).\".format(\n",
    "    len(european_universities), len(european_universities.name.unique())))\n",
    "\n",
    "european_universities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecbb8d-51a2-4613-9fe8-b7a419880819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f13db-edd5-4525-8dd7-9d5543b4d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium \n",
    "from folium import Marker\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a290473-4244-41f3-bd93-800d5d947178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the embed_map() function to visualize your maps.\n",
    "def embed_map(m, file_name):\n",
    "    from IPython.display import IFrame\n",
    "    m.save(file_name)\n",
    "    return IFrame(file_name, width='100%', height='500px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d82428-3150-4a30-a5c0-97ef1ab87146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Geocode the missing locations.\n",
    "# create a DataFrame starbucks containing Starbucks locations in the state of California.\n",
    "\n",
    "# Load and preview Starbucks locations in California\n",
    "starbucks = pd.read_csv(\"../input/geospatial-learn-course-data/starbucks_locations.csv\")\n",
    "starbucks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf9d52-5d6e-453f-aefb-7af41f5a887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n",
    "\n",
    "# How many rows in each column have missing values?\n",
    "print(starbucks.isnull().sum())\n",
    "\n",
    "# View rows with missing locations\n",
    "rows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\n",
    "rows_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431ae01-8404-45d0-938b-2f09b274be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use the code cell below to fill in these values with the Nominatim geocoder.\n",
    "\n",
    "Note that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\n",
    "\n",
    "In this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\n",
    "\n",
    "So, in other words, as long as:\n",
    "\n",
    "you don't change the import statements at the top of the notebook, and\n",
    "you call the geocoding function as geocode() in the code cell below,\n",
    "your code will work as intended!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970d9df-85c1-43c1-8db5-20cf23dd50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the geocoder\n",
    "geolocator = Nominatim(user_agent=\"kaggle_learn\")\n",
    "\n",
    "def my_geocoder(row):\n",
    "    point = geolocator.geocode(row).point\n",
    "    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n",
    "\n",
    "berkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\n",
    "starbucks.update(berkeley_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b94050-4901-4539-81f7-1c27cd559bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) View Berkeley locations.Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\n",
    "\n",
    "# Create a base map\n",
    "m_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n",
    "\n",
    "# Add a marker for each Berkeley location\n",
    "for idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n",
    "    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n",
    "\n",
    "# Show the map\n",
    "embed_map(m_2, 'q_2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74e76f-0d6b-459b-8929-d952025b05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Consolidate your data.\n",
    "\n",
    "CA_counties = gpd.read_file(\"../input/geospatial-learn-course-data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\n",
    "CA_counties.crs = {'init': 'epsg:4326'}\n",
    "CA_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac535308-7b14-483a-89ba-89c4989cbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Next, we create three DataFrames:\n",
    "\n",
    "CA_pop contains an estimate of the population of each county.\n",
    "CA_high_earners contains the number of households with an income of at least $150,000 per year.\n",
    "CA_median_age contains the median age for each county.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe22112-ba3e-4944-a7da-64e09aaaa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_pop = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_population.csv\", index_col=\"GEOID\")\n",
    "CA_high_earners = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_high_earners.csv\", index_col=\"GEOID\")\n",
    "CA_median_age = pd.read_csv(\"../input/geospatial-learn-course-data/CA_county_median_age.csv\", index_col=\"GEOID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab255af-986f-40df-8bce-9d5512389a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\n",
    "# Name the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: \"GEOID\", \"name\", \"area_sqkm\", \"geometry\", \"population\", \"high_earners\", and \"median_age\".\n",
    "\n",
    "cols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\n",
    "CA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ff4e9-363b-4d0d-8cb0-7f2ad752c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all of the data in one place, it's much easier to calculate statistics that use a combination of columns. Create a \"density\" column with the population density.\n",
    "\n",
    "CA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e02644-101a-4013-9006-7e086b174c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''4) Which counties look promising?\n",
    "Collapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\n",
    "\n",
    "Use the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where:\n",
    "\n",
    "there are at least 100,000 households making $150,000 per year,\n",
    "the median age is less than 38.5, and\n",
    "the density of inhabitants is at least 285 (per square kilometer).\n",
    "Additionally, selected counties should satisfy at least one of the following criteria:\n",
    "\n",
    "there are at least 500,000 households making $150,000 per year,\n",
    "the median age is less than 35.5, or\n",
    "the density of inhabitants is at least 1400 (per square kilometer).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed01991-34c9-4cb1-bf5c-8b8e81660a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_counties = CA_stats[((CA_stats.high_earners > 100000) &\n",
    "                         (CA_stats.median_age < 38.5) &\n",
    "                         (CA_stats.density > 285) &\n",
    "                         ((CA_stats.median_age < 35.5) |\n",
    "                         (CA_stats.density > 1400) |\n",
    "                         (CA_stats.high_earners > 500000)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d73ee-7b49-463c-86a2-6ca0f4bfbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) How many stores did you identify?\n",
    "# create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n",
    "\n",
    "starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\n",
    "starbucks_gdf.crs = {'init': 'epsg:4326'}\n",
    "\n",
    "locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\n",
    "num_stores = len(locations_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6c367-48d7-4e0f-9670-8bdce7949f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Visualize the store locations.\n",
    "# Create a map that shows the locations of the stores that you identified in the previous question.\n",
    "\n",
    "# Create a base map\n",
    "m_6 = folium.Map(location=[37,-120], zoom_start=6)\n",
    "\n",
    "# Show selected store locations\n",
    "mc = MarkerCluster()\n",
    "\n",
    "locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\n",
    "for idx, row in locations_of_interest.iterrows():\n",
    "    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n",
    "        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n",
    "\n",
    "m_6.add_child(mc)\n",
    "\n",
    "# Show the map\n",
    "embed_map(m_6, 'q_6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8b1f4-952f-4c29-b8aa-6e7724a845b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351a12d-4854-4cc6-bb9d-1bc21c7c3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af71d8-106f-4016-8ff6-65a1ca8943ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Marker, GeoJson\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71197e-1765-4ccc-9df0-a3166fc2cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dataset from the US Environmental Protection Agency (EPA) that tracks releases of toxic chemicals in Philadelphia, Pennsylvania, USA.\n",
    "\n",
    "releases = gpd.read_file(\"../input/geospatial-learn-course-data/toxic_release_pennsylvania/toxic_release_pennsylvania/toxic_release_pennsylvania.shp\") \n",
    "releases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f22a4-c903-41b4-ad36-d5294606fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dataset that contains readings from air quality monitoring stations in the same city.\n",
    "\n",
    "stations = gpd.read_file(\"../input/geospatial-learn-course-data/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations.shp\")\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af305f-b0e8-4b8b-a35f-512a9c6cc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Measuring distance\n",
    "To measure distances between points from two different GeoDataFrames, we first have to make sure that they use the same coordinate reference system (CRS). Thankfully, this is the case here, where both use EPSG 2272.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccc509-7cf3-4874-a594-ffa3d82f06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stations.crs)\n",
    "print(releases.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b967ffa-98ec-468a-abc9-9afd0cf014d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the distance (in feet) between a relatively recent release incident in recent_release and every station in the stations GeoDataFrame.\n",
    "\n",
    "# Select one release incident in particular\n",
    "recent_release = releases.iloc[360]\n",
    "\n",
    "# Measure distance from release to each station\n",
    "distances = stations.geometry.distance(recent_release.geometry)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559562e-d8d2-41d4-a1b0-2e0e179bfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the calculated distances, we can obtain statistics like the mean distance to each station.\n",
    "\n",
    "print('Mean distance to monitoring stations: {} feet'.format(distances.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8d592-88b0-427e-a002-97c7533f1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, we can get the closest monitoring station\n",
    "\n",
    "print('Closest monitoring station ({} feet):'.format(distances.min()))\n",
    "print(stations.iloc[distances.idxmin()][[\"ADDRESS\", \"LATITUDE\", \"LONGITUDE\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3f9d0-afe0-4495-b22d-1e2b64a733b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a buffer\n",
    "# If we want to understand all points on a map that are some radius away from a point, the simplest way is to create a buffer.\n",
    "# The code cell below creates a GeoSeries two_mile_buffer containing 12 different Polygon objects. Each polygon is a buffer of 2 miles (or, 2*5280 feet) around a different air monitoring station.\n",
    "\n",
    "two_mile_buffer = stations.geometry.buffer(2*5280)\n",
    "two_mile_buffer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2151ff6-7b7e-49cd-8c50-b70abd24aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use folium.GeoJson() to plot each polygon on a map. Note that since folium requires coordinates in latitude and longitude, we have to convert the CRS to EPSG 4326 before plotting.\n",
    "\n",
    "# Create map with release incidents and monitoring stations\n",
    "m = folium.Map(location=[39.9526,-75.1652], zoom_start=11)\n",
    "HeatMap(data=releases[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m)\n",
    "for idx, row in stations.iterrows():\n",
    "    Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)\n",
    "    \n",
    "# Plot each polygon on the map\n",
    "GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m)\n",
    "\n",
    "# Show the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2f861-b1bd-4b66-98c7-d952bbe6c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now, to test if a toxic release occurred within 2 miles of any monitoring station, we could run 12 different tests for each polygon (to check individually if it contains the point).\n",
    "\n",
    "But a more efficient way is to first collapse all of the polygons into a MultiPolygon object. We do this with the unary_union attribute.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032f0e1-b712-4c2e-81c7-8ff83f850e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn group of polygons into single multipolygon\n",
    "my_union = two_mile_buffer.geometry.unary_union\n",
    "print('Type:', type(my_union))\n",
    "\n",
    "# Show the MultiPolygon object\n",
    "my_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba93e47-3c4c-42b9-a4b6-b9fe1d9886c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We use the contains() method to check if the multipolygon contains a point. We'll use the release incident from earlier in the tutorial, which we know is roughly 3781 feet to the closest monitoring station.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f08514-8e3a-4407-b7f5-409d3f15c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closest station is less than two miles away\n",
    "my_union.contains(releases.iloc[360].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3daba1-3aeb-4330-896d-e6650e385282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But not all releases occured within two miles of an air monitoring station!\n",
    "\n",
    "# The closest station is more than two miles away\n",
    "my_union.contains(releases.iloc[358].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a120f9-e6f2-47d7-88fb-46d5c2a205f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73503d-2016-4e6c-8439-8c57c6f5ed53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563c9e0-0e50-4ed3-87e8-4d5043b9f204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ed8bc-2e30-4b02-9637-9c630e136bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9272d01-233c-4cec-8924-bd60bf6c00cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
